{"cells":[{"cell_type":"markdown","metadata":{"id":"zPuSrx1yVODb"},"source":["## Step 1: Mount the Google Drive on Google Collab Notebook and import the Dataset\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6321,"status":"ok","timestamp":1684510288742,"user":{"displayName":"Vidyashri M H","userId":"09359649204749524371"},"user_tz":-330},"id":"HTIsIhasrmHY","outputId":"67667683-178b-4618-cd3f-5c512ac1756f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"vLleKKtbYVGy"},"source":["After mounting our drive I will locate the folder where our data is stored to use it in our colab notebook."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1684510288744,"user":{"displayName":"Vidyashri M H","userId":"09359649204749524371"},"user_tz":-330},"id":"5F9HPaEprtqo","outputId":"1d2c34f0-44d5-4e49-9971-c721d6525f94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset\t\t\t\t Plant_Leaf_Disease_Prediction-Flask-App\n","Model\t\t\t\t Plant_Leaf_Disease_Prediction.ipynb\n","Plant_Deseases_Prediction.ipynb\n"]}],"source":["# After executing the cell above, Drive\n","# files will be present in \"/content/drive/My Drive/Projects/Plant-Leaf-Disease-Prediction\".\n","!ls \"/content/drive/My Drive/Projects/Plant-Leaf-Disease-Prediction\""]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1684510288744,"user":{"displayName":"Vidyashri M H","userId":"09359649204749524371"},"user_tz":-330},"id":"8xABTt5EAtg2","outputId":"b284ad65-47f4-4e5d-d8bf-80575712f06d"},"outputs":[{"name":"stdout","output_type":"stream","text":["'Corn_(maize)_Common_rust'   Potato_Early_blight   Tomato_Bacterial_spot\n"]}],"source":["!ls \"/content/drive/My Drive/Projects/Plant-Leaf-Disease-Prediction/Dataset\""]},{"cell_type":"markdown","metadata":{"id":"hNZ3SfzVVXSQ"},"source":["## Step 2: Import the required libraires\n"]},{"cell_type":"markdown","metadata":{"id":"7BMZmG5jldCj"},"source":["Next we will import all the required libraries. As we are making a CNN model we will import all the required layers, activations, optimizers, etc.  "]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1684510288745,"user":{"displayName":"Vidyashri M H","userId":"09359649204749524371"},"user_tz":-330},"id":"kVSkISgRvTK8"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.image import imread\n","import cv2\n","import random\n","import os\n","from os import listdir\n","from PIL import Image\n","import tensorflow as tf\n","from keras.preprocessing import image\n","from tensorflow.keras.utils import img_to_array, array_to_img\n","from keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Activation, Flatten, Dropout, Dense\n","from sklearn.model_selection import train_test_split\n","from keras.models import model_from_json\n","from keras.utils import to_categorical "]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1684510288746,"user":{"displayName":"Vidyashri M H","userId":"09359649204749524371"},"user_tz":-330},"id":"NHCMW5hlBY-O","outputId":"414e194e-ca7c-4d64-95af-7bbdc2e04243"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n"]}],"source":["print(tf. __version__)"]},{"cell_type":"markdown","metadata":{"id":"D_bdiu0OaWoV"},"source":["## Step 3:Visualizing the images and Resize images\n"]},{"cell_type":"markdown","metadata":{"id":"HhGPvhb8l7_2"},"source":["Now we will observe some of the images that are their in our dataset. We will plot 12 images here using the matplotlib library."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1tZyyguMldEexHSTYIEuxCH4M1Qq9ZrPE"},"id":"WH1pl8Pmpjbf","outputId":"14fe49bb-114b-492c-b182-36f86182546d"},"outputs":[],"source":["# Plotting 12 images to check dataset\n","plt.figure(figsize=(12,12))\n","path = \"/content/drive/My Drive/Projects/Plant-Leaf-Disease-Prediction/Dataset/Potato_Early_blight\"\n","for i in range(1,17):\n","    plt.subplot(4,4,i)\n","    plt.tight_layout()\n","    rand_img = imread(path +'/'+ random.choice(sorted(os.listdir(path))))\n","    plt.imshow(rand_img)\n","    plt.xlabel(rand_img.shape[1], fontsize = 10)#width of image\n","    plt.ylabel(rand_img.shape[0], fontsize = 10)#height of image"]},{"cell_type":"markdown","metadata":{"id":"es4qR69aaidj"},"source":["## Step 4: Convert the images into a NumPy array and normalize them. \n"]},{"cell_type":"markdown","metadata":{"id":"VPy1pFxa3ZIR"},"source":["After visualizing the images let us move forward and create a function which will convert the images into a numpy array. It is required because we will normalize our dataset after this."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ERSCAiVU5Vbd"},"outputs":[],"source":["#Converting Images to array \n","def convert_image_to_array(image_dir):\n","    try:\n","        image = cv2.imread(image_dir)\n","        if image is not None :\n","            image = cv2.resize(image, (256,256))   \n","            return img_to_array(image)\n","        else :\n","            return np.array([])\n","    except Exception as e:\n","        print(f\"Error : {e}\")\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"ZjEAueGv4yPX"},"source":["Now we will convert all the images into numpy array."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rhte_44Fr43o"},"outputs":[],"source":["dir = \"/content/drive/My Drive/Projects/Plant-Leaf-Disease-Prediction/Dataset\"\n","image_list, label_list = [], []\n","all_labels = ['Tomato-Bacterial_spot', 'Potato-Early_blight', 'Corn-Common_rust']\n","binary_labels = [0,1,2]\n","temp = -1\n","\n","# Reading and converting image to numpy array\n","for directory in ['Tomato_Bacterial_spot', 'Potato_Early_blight', 'Corn_(maize)_Common_rust']:\n","  plant_image_list = listdir(f\"{dir}/{directory}\")\n","  temp += 1\n","  for files in plant_image_list:\n","    image_path = f\"{dir}/{directory}/{files}\"\n","    image_list.append(convert_image_to_array(image_path))\n","    label_list.append(binary_labels[temp])"]},{"cell_type":"markdown","metadata":{"id":"ZqFKp_jGaxZ8"},"source":["## Step 5: Visualize the class count and Check for class imbalance \n"]},{"cell_type":"markdown","metadata":{"id":"gfn1LTwP6qmp"},"source":["We will also observe the number of images under different classes to see if the dataset is balanced or not"]},{"cell_type":"markdown","metadata":{"id":"NuI5Pgoa7lGx"},"source":["Next we will observe the shape of the image."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7GnSjx-aqT6I"},"outputs":[{"data":{"text/plain":["0    300\n","1    300\n","2    300\n","dtype: int64"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# Visualize the number of classes count\n","label_counts = pd.DataFrame(label_list).value_counts()\n","label_counts.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2UPqSYwxqd2j"},"outputs":[{"data":{"text/plain":["(256, 256, 3)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["image_list[0].shape"]},{"cell_type":"markdown","metadata":{"id":"6QmqfbOX7rbi"},"source":["Checking the total number of the images which is the length of the labels list."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"84jyMC3g6A2I"},"outputs":[{"data":{"text/plain":["(900,)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["label_list = np.array(label_list)\n","label_list.shape"]},{"cell_type":"markdown","metadata":{"id":"1IBW4sGIa7YO"},"source":["## Step 6: Splitting the dataset into train, validate and test sets\n"]},{"cell_type":"markdown","metadata":{"id":"R_hPmZcH8xsC"},"source":["Next we will use sklearn train_test_split to split the dataset into testing and training data. Here I have taken test size as 0.2 so my data will be divided into 80% training and 20% testing data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_uj2tiS99gjD"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "]},{"cell_type":"markdown","metadata":{"id":"Mxx3natj8jc9"},"source":["Now we will normalize the dataset of our images. As pixel values ranges from 0 to 255 so we will divide each image pixel with 255 to normalize the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8k79Lrwn9dxs"},"outputs":[],"source":["x_train = np.array(x_train, dtype=np.float16) / 255.0\n","x_test = np.array(x_test, dtype=np.float16) / 255.0\n","x_train = x_train.reshape( -1, 256,256,3)\n","x_test = x_test.reshape( -1, 256,256,3)"]},{"cell_type":"markdown","metadata":{"id":"QHyMUORHbC8h"},"source":["## Step 7: Performing one-hot encoding on target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGh439ZAqK1p"},"outputs":[],"source":["y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"]},{"cell_type":"markdown","metadata":{"id":"pjvpdhvzbOrP"},"source":["## Step 8: Creating the model architecture, compile the model and then fit it using the training data"]},{"cell_type":"markdown","metadata":{"id":"BE4moAAbo9ET"},"source":["Next we will create a network architecture for the model. We have used different types of layers according to their features namely Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggVzpCbb9kp-"},"outputs":[],"source":["model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=(256,256,3), activation=\"relu\"))\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","model.add(Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(8, activation=\"relu\"))\n","model.add(Dense(3, activation=\"softmax\"))\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"cwjP40jjpPo0"},"source":["While compiling the model we need to set the type of loss which will be Binary Crossentropy for our model alongwith this we also need to set the optimizer and the metrics respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSZceAYY-Yu4"},"outputs":[],"source":["model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001),metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"RFPtXVeoqJgv"},"source":["Next we will split the training dataset into validation and training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTHjVTuA_Zjb"},"outputs":[],"source":["# Splitting the training data set into training and validation data sets\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 10)"]},{"cell_type":"markdown","metadata":{"id":"zjVpmPlkqVT6"},"source":["Fitting the model with the data and finding out the accuracy at each epoch to see how our model is learning. Now we will train our model on 10 epochs and a batch size of 128. You can try using more number of epochs to increase accuracy but here we can see that the model has already raeched a very high accuracy so we don't need to run it for more. During each epochs we can see how the model is performing by viewing the training and validation accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9aUasny_DY7"},"outputs":[],"source":["# Training the model\n","epochs = 50\n","batch_size = 128\n","history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))"]},{"cell_type":"markdown","metadata":{"id":"7fuuVxsUJ9Cd"},"source":["Saving the model using different techniques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5Bcb5S6irlP"},"outputs":[],"source":["model.save(\"/content/drive/My Drive/Projects/Plant-Leaf-Disease-Prediction/Model/plant_disease_model.h5\")"]},{"cell_type":"markdown","metadata":{"id":"SFzEqnMRbmsB"},"source":["## Step 9: Plot the accuracy and loss against each epoch"]},{"cell_type":"markdown","metadata":{"id":"dVLGbxJxKCZ2"},"source":["Next we will plot the accuracy of the model for the trainig history."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJmXMcse_i8H"},"outputs":[],"source":["#Plot the training history\n","plt.figure(figsize=(12, 5))\n","plt.plot(history.history['accuracy'], color='r')\n","plt.plot(history.history['val_accuracy'], color='b')\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend(['train', 'val'])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aKXkfQ-bKL8e"},"source":["Evaluating the model to know the accuracy of the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIOUuhjT_tjf"},"outputs":[],"source":["print(\"Calculating model accuracy\")\n","scores = model.evaluate(x_test, y_test)\n","print(f\"Test Accuracy: {scores[1]*100}\")"]},{"cell_type":"markdown","metadata":{"id":"oNz8QPlAb0rO"},"source":["## Step 10: Make predictions on testing data"]},{"cell_type":"markdown","metadata":{"id":"RMMMLTuNKVcU"},"source":["Next we will use our model to predict predicting the testing dataset label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIQQPjLFjXQX"},"outputs":[],"source":["y_pred = model.predict(x_test)"]},{"cell_type":"markdown","metadata":{"id":"tlvBffF_cDJa"},"source":["## Step 11: Visualizing the original and predicted labels for the test images"]},{"cell_type":"markdown","metadata":{"id":"YkJqzX1wKort"},"source":["Printing out the original and the predicted label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rv_hJC19oD7j"},"outputs":[],"source":["# Plotting image to compare\n","img = array_to_img(x_test[11])\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giGnMYE5_t_-"},"outputs":[],"source":["# Finding max value from predition list and comaparing original value vs predicted\n","print(\"Original Label: \",all_labels[np.argmax(y_test[11])])\n","print(\"Predicted Label: \",all_labels[np.argmax(y_pred[4])])\n","print (y_pred[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAWauHkPoZlf"},"outputs":[],"source":["for i in range(50):\n","  print (all_labels[np.argmax(y_test[i])], \"-\", all_labels[np.argmax(y_pred[i])])"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
